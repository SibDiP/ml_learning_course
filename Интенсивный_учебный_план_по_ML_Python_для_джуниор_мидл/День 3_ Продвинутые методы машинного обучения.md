# День 3: Продвинутые методы машинного обучения

## Расписание дня

| Время | Активность |
|-------|------------|
| 09:00 - 10:30 | Теория: Методы снижения размерности |
| 10:30 - 10:45 | Перерыв |
| 10:45 - 12:15 | Теория: Кластеризация и методы отбора признаков |
| 12:15 - 13:15 | Обед |
| 13:15 - 15:00 | Практика: Применение методов снижения размерности и кластеризации |
| 15:00 - 15:15 | Перерыв |
| 15:15 - 17:00 | Практика: Оптимизация гиперпараметров и создание пайплайнов |
| 17:00 - 18:00 | Работа над домашним заданием и вопросы |

## Теоретические материалы

### Методы снижения размерности

#### Метод главных компонент (PCA)
- Основные принципы и математическая формулировка
- Собственные значения и собственные векторы
- Объясненная дисперсия и выбор количества компонент
- Применение PCA для визуализации и предобработки данных
- Ограничения PCA и когда его использовать

#### t-SNE (t-distributed Stochastic Neighbor Embedding)
- Принцип работы t-SNE
- Отличия от PCA
- Параметры t-SNE и их влияние на результат
- Применение t-SNE для визуализации высокоразмерных данных
- Ограничения t-SNE

#### UMAP (Uniform Manifold Approximation and Projection)
- Принцип работы UMAP
- Сравнение с t-SNE и PCA
- Параметры UMAP и их настройка
- Применение UMAP для визуализации и снижения размерности

### Кластеризация

#### K-means
- Принцип работы алгоритма K-means
- Выбор оптимального числа кластеров (метод локтя, силуэтный анализ)
- Инициализация центроидов (K-means++)
- Преимущества и ограничения K-means

#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
- Принцип работы DBSCAN
- Параметры eps и min_samples
- Преимущества DBSCAN: обнаружение кластеров произвольной формы и выбросов
- Ограничения DBSCAN

#### Иерархическая кластеризация
- Агломеративные и дивизивные методы
- Метрики расстояния и методы связи
- Дендрограммы и их интерпретация
- Преимущества и ограничения иерархической кластеризации

### Методы отбора и генерации признаков

#### Отбор признаков
- Фильтрация признаков (Filter methods)
  - Корреляционный анализ
  - Дисперсионный анализ (ANOVA)
  - Информационная ценность (Information Gain)
- Оберточные методы (Wrapper methods)
  - Прямой отбор (Forward Selection)
  - Обратное исключение (Backward Elimination)
  - Рекурсивное исключение признаков (RFE)
- Встроенные методы (Embedded methods)
  - Регуляризация (L1, L2)
  - Важность признаков в деревьях решений

#### Генерация признаков
- Полиномиальные признаки
- Взаимодействия признаков
- Агрегация признаков
- Преобразования признаков (логарифмирование, возведение в степень и т.д.)
- Автоматическая генерация признаков

### Оптимизация гиперпараметров

#### Методы оптимизации
- Grid Search (поиск по сетке)
- Random Search (случайный поиск)
- Байесовская оптимизация
- Генетические алгоритмы

#### Стратегии оптимизации
- Выбор метрики для оптимизации
- Кросс-валидация при оптимизации
- Вложенная кросс-валидация
- Параллельная оптимизация

## Практические задания

### Задание 1: Применение методов снижения размерности

#### PCA для снижения размерности и визуализации
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_digits

# Загрузка данных (например, Digits dataset)
digits = load_digits()
X = digits.data
y = digits.target

# Масштабирование данных
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Применение PCA
pca = PCA()
X_pca = pca.fit_transform(X_scaled)

# Анализ объясненной дисперсии
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance_ratio = np.cumsum(explained_variance_ratio)

# Визуализация объясненной дисперсии
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='-')
plt.xlabel('Количество компонент')
plt.ylabel('Объясненная дисперсия')
plt.title('Объясненная дисперсия в зависимости от количества компонент')
plt.grid(True)
plt.show()

# Определение оптимального числа компонент (например, 95% объясненной дисперсии)
n_components = np.argmax(cumulative_variance_ratio >= 0.95) + 1
print(f"Оптимальное количество компонент для сохранения 95% дисперсии: {n_components}")

# Применение PCA с оптимальным числом компонент
pca = PCA(n_components=n_components)
X_pca_reduced = pca.fit_transform(X_scaled)
print(f"Исходная размерность: {X.shape[1]}, Новая размерность: {X_pca_reduced.shape[1]}")

# Визуализация данных в пространстве первых двух главных компонент
pca_2d = PCA(n_components=2)
X_pca_2d = pca_2d.fit_transform(X_scaled)

plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y, cmap='viridis', alpha=0.8, edgecolors='w')
plt.colorbar(scatter, label='Digit')
plt.xlabel('Первая главная компонента')
plt.ylabel('Вторая главная компонента')
plt.title('Визуализация набора данных Digits с помощью PCA')
plt.grid(True)
plt.show()
```

#### t-SNE для визуализации высокоразмерных данных
```python
from sklearn.manifold import TSNE

# Применение t-SNE
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
X_tsne = tsne.fit_transform(X_scaled)

# Визуализация результатов t-SNE
plt.figure(figsize=(12, 10))
scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.8, edgecolors='w')
plt.colorbar(scatter, label='Digit')
plt.xlabel('t-SNE компонента 1')
plt.ylabel('t-SNE компонента 2')
plt.title('Визуализация набора данных Digits с помощью t-SNE')
plt.grid(True)
plt.show()

# Сравнение различных значений perplexity
perplexities = [5, 30, 50, 100]
fig, axes = plt.subplots(2, 2, figsize=(15, 12))
axes = axes.flatten()

for i, perplexity in enumerate(perplexities):
    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity, n_iter=1000)
    X_tsne = tsne.fit_transform(X_scaled)
    
    scatter = axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', alpha=0.8, edgecolors='w')
    axes[i].set_title(f't-SNE с perplexity={perplexity}')
    axes[i].set_xlabel('t-SNE компонента 1')
    axes[i].set_ylabel('t-SNE компонента 2')
    axes[i].grid(True)

plt.tight_layout()
plt.show()
```

### Задание 2: Реализация алгоритмов кластеризации

#### K-means кластеризация
```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.datasets import load_iris

# Загрузка данных (например, Iris dataset)
iris = load_iris()
X = iris.data
y = iris.target

# Масштабирование данных
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Определение оптимального числа кластеров с помощью метода локтя
inertia = []
silhouette_scores = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)
    
    # Вычисление силуэтного коэффициента
    if k > 1:  # Силуэтный коэффициент требует минимум 2 кластера
        labels = kmeans.labels_
        silhouette_scores.append(silhouette_score(X_scaled, labels))

# Визуализация метода локтя
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(k_range, inertia, marker='o', linestyle='-')
plt.xlabel('Количество кластеров')
plt.ylabel('Инерция')
plt.title('Метод локтя для определения оптимального k')
plt.grid(True)

plt.subplot(1, 2, 2)
plt.plot(k_range[1:], silhouette_scores, marker='o', linestyle='-')
plt.xlabel('Количество кластеров')
plt.ylabel('Силуэтный коэффициент')
plt.title('Силуэтный анализ для определения оптимального k')
plt.grid(True)

plt.tight_layout()
plt.show()

# Применение K-means с оптимальным числом кластеров (например, k=3 для Iris)
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(X_scaled)

# Визуализация результатов кластеризации
plt.figure(figsize=(12, 5))

# Визуализация в пространстве первых двух признаков
plt.subplot(1, 2, 1)
scatter = plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis', alpha=0.8, edgecolors='w')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Центроиды')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.title('K-means кластеризация (первые два признака)')
plt.legend()
plt.grid(True)

# Визуализация в пространстве PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.8, edgecolors='w')
plt.xlabel('PCA компонента 1')
plt.ylabel('PCA компонента 2')
plt.title('K-means кластеризация (PCA)')
plt.grid(True)

plt.tight_layout()
plt.show()

# Сравнение с истинными метками
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

ari = adjusted_rand_score(y, cluster_labels)
nmi = normalized_mutual_info_score(y, cluster_labels)

print(f"Adjusted Rand Index: {ari:.4f}")
print(f"Normalized Mutual Information: {nmi:.4f}")
```

#### DBSCAN кластеризация
```python
from sklearn.cluster import DBSCAN
from sklearn.neighbors import NearestNeighbors

# Определение оптимального значения eps с помощью графика k-расстояний
k = 5  # Количество соседей
neigh = NearestNeighbors(n_neighbors=k)
neigh.fit(X_scaled)
distances, indices = neigh.kneighbors(X_scaled)
distances = np.sort(distances[:, k-1])

plt.figure(figsize=(10, 6))
plt.plot(distances)
plt.xlabel('Точки данных (отсортированные по расстоянию)')
plt.ylabel(f'Расстояние до {k}-го ближайшего соседа')
plt.title('График k-расстояний для определения eps')
plt.grid(True)
plt.show()

# Применение DBSCAN с выбранными параметрами
eps = 0.5  # Выбрать значение на основе графика k-расстояний
min_samples = 5
dbscan = DBSCAN(eps=eps, min_samples=min_samples)
dbscan_labels = dbscan.fit_predict(X_scaled)

# Визуализация результатов DBSCAN
plt.figure(figsize=(12, 5))

# Визуализация в пространстве первых двух признаков
plt.subplot(1, 2, 1)
scatter = plt.scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.8, edgecolors='w')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.title('DBSCAN кластеризация (первые два признака)')
plt.grid(True)

# Визуализация в пространстве PCA
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap='viridis', alpha=0.8, edgecolors='w')
plt.xlabel('PCA компонента 1')
plt.ylabel('PCA компонента 2')
plt.title('DBSCAN кластеризация (PCA)')
plt.grid(True)

plt.tight_layout()
plt.show()

# Анализ результатов DBSCAN
n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)
n_noise = list(dbscan_labels).count(-1)
print(f"Количество кластеров: {n_clusters}")
print(f"Количество выбросов: {n_noise}")
```

#### Иерархическая кластеризация
```python
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage

# Применение иерархической кластеризации
n_clusters = 3
agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)
agg_labels = agg_clustering.fit_predict(X_scaled)

# Построение дендрограммы
# Примечание: для больших наборов данных может потребоваться выборка
linkage_matrix = linkage(X_scaled, method='ward')

plt.figure(figsize=(15, 10))
dendrogram(linkage_matrix)
plt.title('Дендрограмма иерархической кластеризации')
plt.xlabel('Индексы образцов')
plt.ylabel('Расстояние')
plt.axhline(y=6, color='r', linestyle='--')  # Линия для выбора количества кластеров
plt.show()

# Визуализация результатов иерархической кластеризации
plt.figure(figsize=(12, 5))

# Визуализация в пространстве первых двух признаков
plt.subplot(1, 2, 1)
scatter = plt.scatter(X[:, 0], X[:, 1], c=agg_labels, cmap='viridis', alpha=0.8, edgecolors='w')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.title('Иерархическая кластеризация (первые два признака)')
plt.grid(True)

# Визуализация в пространстве PCA
plt.subplot(1, 2, 2)
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=agg_labels, cmap='viridis', alpha=0.8, edgecolors='w')
plt.xlabel('PCA компонента 1')
plt.ylabel('PCA компонента 2')
plt.title('Иерархическая кластеризация (PCA)')
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Задание 3: Отбор признаков и оптимизация гиперпараметров

#### Отбор признаков
```python
from sklearn.feature_selection import SelectKBest, f_classif, RFE
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

# Загрузка данных (например, Breast Cancer dataset)
from sklearn.datasets import load_breast_cancer
cancer = load_breast_cancer()
X = cancer.data
y = cancer.target
feature_names = cancer.feature_names

# 1. Фильтрация признаков с помощью ANOVA F-value
k_best = 10  # Выбрать 10 лучших признаков
selector = SelectKBest(f_classif, k=k_best)
X_kbest = selector.fit_transform(X, y)

# Визуализация важности признаков
scores = selector.scores_
feature_scores = pd.DataFrame({
    'Feature': feature_names,
    'Score': scores
}).sort_values('Score', ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x='Score', y='Feature', data=feature_scores.head(15))
plt.title('Важность признаков (ANOVA F-value)')
plt.tight_layout()
plt.show()

# 2. Рекурсивное исключение признаков (RFE)
estimator = LogisticRegression(solver='liblinear')
rfe = RFE(estimator=estimator, n_features_to_select=k_best, step=1)
X_rfe = rfe.fit_transform(X, y)

# Визуализация выбранных признаков
feature_ranking = pd.DataFrame({
    'Feature': feature_names,
    'Ranking': rfe.ranking_
}).sort_values('Ranking')

plt.figure(figsize=(12, 6))
sns.barplot(x='Ranking', y='Feature', data=feature_ranking.head(15))
plt.title('Ранжирование признаков (RFE)')
plt.tight_layout()
plt.show()

# 3. Важность признаков на основе случайного леса
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X, y)

feature_importance = pd.DataFrame({
    'Feature': feature_names,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))
plt.title('Важность признаков (Random Forest)')
plt.tight_layout()
plt.show()
```

#### Оптимизация гиперпараметров
```python
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Grid Search
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Определение нескольких метрик для оценки
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'precision': make_scorer(precision_score),
    'recall': make_scorer(recall_score),
    'f1': make_scorer(f1_score)
}

# Grid Search с кросс-валидацией
grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    scoring=scoring,
    refit='f1',  # Оптимизация по F1-мере
    cv=5,
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

# Вывод результатов
print(f"Лучшие параметры: {grid_search.best_params_}")
print(f"Лучший результат (F1): {grid_search.best_score_:.4f}")

# Оценка на тестовой выборке
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"Accuracy на тестовой выборке: {accuracy_score(y_test, y_pred):.4f}")
print(f"Precision на тестовой выборке: {precision_score(y_test, y_pred):.4f}")
print(f"Recall на тестовой выборке: {recall_score(y_test, y_pred):.4f}")
print(f"F1 на тестовой выборке: {f1_score(y_test, y_pred):.4f}")

# 2. Random Search (для сравнения)
from scipy.stats import randint

param_dist = {
    'n_estimators': randint(50, 300),
    'max_depth': [None] + list(randint(5, 50).rvs(5)),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10)
}

random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=20,  # Количество комбинаций для проверки
    scoring='f1',
    cv=5,
    verbose=1,
    random_state=42,
    n_jobs=-1
)

random_search.fit(X_train, y_train)

print(f"Лучшие параметры (Random Search): {random_search.best_params_}")
print(f"Лучший результат (F1): {random_search.best_score_:.4f}")
```

### Задание 4: Создание пайплайнов обработки данных и обучения моделей

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Загрузка данных (например, Adult dataset)
# Для примера создадим синтетические данные с числовыми и категориальными признаками
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, 
                           n_redundant=2, n_repeated=0, n_classes=2, 
                           random_state=42)

# Добавим категориальные признаки
import pandas as pd
X_df = pd.DataFrame(X, columns=[f'num_{i}' for i in range(X.shape[1])])
X_df['cat_1'] = np.random.choice(['A', 'B', 'C'], size=X_df.shape[0])
X_df['cat_2'] = np.random.choice(['X', 'Y', 'Z', 'W'], size=X_df.shape[0])

# Добавим пропущенные значения
for col in X_df.columns:
    mask = np.random.random(X_df.shape[0]) < 0.05  # 5% пропущенных значений
    X_df.loc[mask, col] = np.nan

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.2, random_state=42)

# Определение числовых и категориальных признаков
numeric_features = [col for col in X_df.columns if col.startswith('num_')]
categorical_features = [col for col in X_df.columns if col.startswith('cat_')]

# Создание преобразователей для разных типов признаков
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Объединение преобразователей с помощью ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Создание полного пайплайна: предобработка + модель
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Обучение пайплайна
pipeline.fit(X_train, y_train)

# Оценка пайплайна
accuracy = pipeline.score(X_test, y_test)
print(f"Accuracy пайплайна: {accuracy:.4f}")

# Оптимизация гиперпараметров пайплайна
param_grid = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [None, 10, 20],
    'preprocessor__num__imputer__strategy': ['mean', 'median']
}

grid_search = GridSearchCV(
    pipeline,
    param_grid=param_grid,
    cv=5,
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print(f"Лучшие параметры пайплайна: {grid_search.best_params_}")
print(f"Лучший результат: {grid_search.best_score_:.4f}")

# Оценка оптимизированного пайплайна на тестовой выборке
best_pipeline = grid_search.best_estimator_
accuracy = best_pipeline.score(X_test, y_test)
print(f"Accuracy оптимизированного пайплайна: {accuracy:.4f}")
```

## Домашнее задание

### Задание 1: Решение задачи кластеризации на реальном наборе данных

Используя набор данных "Mall Customer Segmentation" (или другой предложенный набор):

1. Загрузите данные и выполните необходимую предобработку
2. Проведите разведочный анализ данных для понимания структуры
3. Примените методы снижения размерности (PCA, t-SNE) для визуализации данных
4. Реализуйте и сравните различные алгоритмы кластеризации:
   - K-means
   - DBSCAN
   - Иерархическая кластеризация
5. Для каждого алгоритма:
   - Определите оптимальное количество кластеров или параметры
   - Визуализируйте результаты кластеризации
   - Проанализируйте характеристики полученных кластеров
6. Сделайте выводы о сегментации клиентов и предложите рекомендации

### Задание 2: Оптимизация гиперпараметров модели для повышения качества

1. Выберите набор данных для классификации или регрессии
2. Разделите данные на обучающую и тестовую выборки
3. Создайте пайплайн, включающий:
   - Предобработку данных (обработка пропусков, масштабирование, кодирование)
   - Отбор признаков
   - Модель машинного обучения (например, RandomForest или GradientBoosting)
4. Проведите оптимизацию гиперпараметров с помощью:
   - Grid Search
   - Random Search
5. Сравните результаты разных подходов к оптимизации
6. Проанализируйте важность различных гиперпараметров
7. Оцените итоговую модель на тестовой выборке и сделайте выводы

## Ресурсы и материалы

### Основные ресурсы
- [Документация scikit-learn по снижению размерности](https://scikit-learn.org/stable/modules/decomposition.html)
- [Документация scikit-learn по кластеризации](https://scikit-learn.org/stable/modules/clustering.html)
- [Документация scikit-learn по отбору признаков](https://scikit-learn.org/stable/modules/feature_selection.html)
- [Документация scikit-learn по оптимизации гиперпараметров](https://scikit-learn.org/stable/modules/grid_search.html)
- [Документация scikit-learn по пайплайнам](https://scikit-learn.org/stable/modules/compose.html)

### Дополнительные материалы
- [Статья "How to Use t-SNE Effectively"](https://distill.pub/2016/misread-tsne/)
- [Статья "Understanding UMAP"](https://pair-code.github.io/understanding-umap/)
- [Статья "A Comprehensive Guide to Feature Selection"](https://towardsdatascience.com/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python-de6399f1986d)
- [Статья "Hyperparameter Tuning the Random Forest in Python"](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)
- [Книга "Feature Engineering for Machine Learning"](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)

### Наборы данных для практики
- [Mall Customer Segmentation Data](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python)
- [Wine Quality Dataset](https://archive.ics.uci.edu/ml/datasets/wine+quality)
- [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)


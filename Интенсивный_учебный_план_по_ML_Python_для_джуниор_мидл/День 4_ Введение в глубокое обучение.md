# День 4: Введение в глубокое обучение

## Расписание дня

| Время | Активность |
|-------|------------|
| 09:00 - 10:30 | Теория: Основы нейронных сетей |
| 10:30 - 10:45 | Перерыв |
| 10:45 - 12:15 | Теория: Архитектуры нейронных сетей и методы оптимизации |
| 12:15 - 13:15 | Обед |
| 13:15 - 15:00 | Практика: Реализация простой нейронной сети с TensorFlow/Keras |
| 15:00 - 15:15 | Перерыв |
| 15:15 - 17:00 | Практика: Визуализация и регуляризация нейронных сетей |
| 17:00 - 18:00 | Работа над домашним заданием и вопросы |

## Теоретические материалы

### Основы нейронных сетей

#### Биологические основы и история
- Нейрон как базовый элемент нервной системы
- История развития искусственных нейронных сетей
- Перцептрон Розенблатта
- Многослойный перцептрон (MLP)

#### Структура искусственного нейрона
- Входные сигналы и веса
- Функция активации
- Выходной сигнал
- Математическая модель нейрона

#### Функции активации
- Линейная функция
- Сигмоидальная функция
- Гиперболический тангенс
- ReLU (Rectified Linear Unit) и его варианты
- Softmax для многоклассовой классификации

#### Архитектура многослойной нейронной сети
- Входной слой
- Скрытые слои
- Выходной слой
- Полносвязные (dense) слои
- Прямое распространение сигнала (forward propagation)

### Обучение нейронных сетей

#### Функции потерь
- Среднеквадратическая ошибка (MSE)
- Перекрестная энтропия (Cross-Entropy)
- Бинарная перекрестная энтропия
- Категориальная перекрестная энтропия
- Функция Хьюбера (Huber loss)

#### Алгоритм обратного распространения ошибки
- Градиентный спуск
- Стохастический градиентный спуск (SGD)
- Мини-батч градиентный спуск
- Обратное распространение ошибки (backpropagation)
- Цепное правило дифференцирования

#### Оптимизаторы
- Градиентный спуск (GD)
- Стохастический градиентный спуск (SGD)
- Momentum
- RMSprop
- Adam и AdamW
- Сравнение оптимизаторов

#### Методы регуляризации
- L1 и L2 регуляризация (Lasso и Ridge)
- Dropout
- Batch Normalization
- Early Stopping
- Data Augmentation

### Архитектуры нейронных сетей для различных задач

#### Нейронные сети для классификации
- Бинарная классификация
- Многоклассовая классификация
- Архитектура выходного слоя
- Метрики оценки качества

#### Нейронные сети для регрессии
- Особенности архитектуры
- Функции потерь для регрессии
- Метрики оценки качества

#### Нейронные сети для многозадачного обучения
- Общие скрытые слои
- Специфичные для задач выходные слои
- Балансировка функций потерь

## Практические задания

### Задание 1: Реализация простой нейронной сети с TensorFlow/Keras

#### Установка и импорт библиотек
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Установка TensorFlow и Keras (если не установлены)
!pip install tensorflow

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.utils import plot_model

print(f"TensorFlow version: {tf.__version__}")
print(f"Keras version: {keras.__version__}")
```

#### Подготовка данных для классификации
```python
# Загрузка набора данных MNIST
from tensorflow.keras.datasets import mnist

# Загрузка данных
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Просмотр формы данных
print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# Визуализация нескольких примеров
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i+1)
    plt.imshow(X_train[i], cmap='gray')
    plt.title(f"Label: {y_train[i]}")
    plt.axis('off')
plt.tight_layout()
plt.show()

# Предобработка данных
# 1. Нормализация значений пикселей
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# 2. Преобразование меток в one-hot encoding
y_train_onehot = keras.utils.to_categorical(y_train, 10)
y_test_onehot = keras.utils.to_categorical(y_test, 10)

# 3. Изменение формы входных данных для полносвязной сети
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)
print(f"X_train_flat shape: {X_train_flat.shape}")
```

#### Создание и обучение простой полносвязной нейронной сети
```python
# Создание модели
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(784,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Компиляция модели
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Вывод структуры модели
model.summary()

# Визуализация архитектуры модели
plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)

# Обучение модели
history = model.fit(
    X_train_flat, y_train_onehot,
    epochs=10,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

# Оценка модели на тестовой выборке
test_loss, test_acc = model.evaluate(X_test_flat, y_test_onehot)
print(f"Test accuracy: {test_acc:.4f}")
```

#### Визуализация процесса обучения
```python
# Визуализация процесса обучения
plt.figure(figsize=(12, 5))

# График точности
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# График функции потерь
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

#### Предсказания и визуализация результатов
```python
# Получение предсказаний
y_pred_proba = model.predict(X_test_flat)
y_pred = np.argmax(y_pred_proba, axis=1)

# Визуализация некоторых предсказаний
plt.figure(figsize=(12, 8))
for i in range(15):
    plt.subplot(3, 5, i+1)
    plt.imshow(X_test[i], cmap='gray')
    pred_label = y_pred[i]
    true_label = y_test[i]
    color = 'green' if pred_label == true_label else 'red'
    plt.title(f"Pred: {pred_label}, True: {true_label}", color=color)
    plt.axis('off')
plt.tight_layout()
plt.show()

# Матрица ошибок
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Отчет о классификации
print(classification_report(y_test, y_pred))
```

### Задание 2: Реализация нейронной сети с использованием функционального API Keras

```python
# Создание более сложной модели с использованием функционального API
inputs = keras.Input(shape=(784,))
x = layers.Dense(256, activation='relu')(inputs)
x = layers.Dropout(0.3)(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dropout(0.3)(x)
outputs = layers.Dense(10, activation='softmax')(x)

functional_model = keras.Model(inputs=inputs, outputs=outputs, name="mnist_functional_model")

# Компиляция модели
functional_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Вывод структуры модели
functional_model.summary()

# Визуализация архитектуры модели
plot_model(functional_model, to_file='functional_model_architecture.png', show_shapes=True, show_layer_names=True)

# Обучение модели с использованием callbacks
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=2
)

history_functional = functional_model.fit(
    X_train_flat, y_train_onehot,
    epochs=20,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Оценка модели на тестовой выборке
test_loss, test_acc = functional_model.evaluate(X_test_flat, y_test_onehot)
print(f"Test accuracy: {test_acc:.4f}")
```

### Задание 3: Применение методов регуляризации для борьбы с переобучением

```python
# Создание модели с различными методами регуляризации
regularized_model = keras.Sequential([
    layers.Dense(256, activation='relu', input_shape=(784,),
                kernel_regularizer=keras.regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(128, activation='relu',
                kernel_regularizer=keras.regularizers.l2(0.001)),
    layers.BatchNormalization(),
    layers.Dropout(0.3),
    layers.Dense(10, activation='softmax')
])

# Компиляция модели
regularized_model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Вывод структуры модели
regularized_model.summary()

# Обучение модели
history_regularized = regularized_model.fit(
    X_train_flat, y_train_onehot,
    epochs=20,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

# Оценка модели на тестовой выборке
test_loss, test_acc = regularized_model.evaluate(X_test_flat, y_test_onehot)
print(f"Test accuracy: {test_acc:.4f}")

# Сравнение моделей
plt.figure(figsize=(12, 5))

# График точности
plt.subplot(1, 2, 1)
plt.plot(history.history['val_accuracy'], label='Basic Model')
plt.plot(history_functional.history['val_accuracy'], label='Functional Model')
plt.plot(history_regularized.history['val_accuracy'], label='Regularized Model')
plt.title('Validation Accuracy Comparison')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# График функции потерь
plt.subplot(1, 2, 2)
plt.plot(history.history['val_loss'], label='Basic Model')
plt.plot(history_functional.history['val_loss'], label='Functional Model')
plt.plot(history_regularized.history['val_loss'], label='Regularized Model')
plt.title('Validation Loss Comparison')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
```

### Задание 4: Реализация нейронной сети для регрессии

```python
# Загрузка набора данных для регрессии (например, Boston Housing)
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Загрузка данных
boston = load_boston()
X = boston.data
y = boston.target

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Масштабирование признаков
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Создание модели для регрессии
regression_model = keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    layers.Dense(32, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1)  # Линейная активация для регрессии
])

# Компиляция модели
regression_model.compile(
    optimizer='adam',
    loss='mse',  # Mean Squared Error для регрессии
    metrics=['mae']  # Mean Absolute Error
)

# Вывод структуры модели
regression_model.summary()

# Обучение модели
history_regression = regression_model.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=16,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

# Оценка модели на тестовой выборке
test_loss, test_mae = regression_model.evaluate(X_test_scaled, y_test)
print(f"Test MAE: {test_mae:.4f}")

# Визуализация процесса обучения
plt.figure(figsize=(12, 5))

# График MAE
plt.subplot(1, 2, 1)
plt.plot(history_regression.history['mae'], label='Train')
plt.plot(history_regression.history['val_mae'], label='Validation')
plt.title('Model MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()
plt.grid(True)

# График функции потерь
plt.subplot(1, 2, 2)
plt.plot(history_regression.history['loss'], label='Train')
plt.plot(history_regression.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Предсказания и визуализация результатов
y_pred = regression_model.predict(X_test_scaled).flatten()

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Values')
plt.grid(True)
plt.show()

# Распределение ошибок
errors = y_pred - y_test
plt.figure(figsize=(10, 6))
plt.hist(errors, bins=20)
plt.xlabel('Prediction Error')
plt.ylabel('Count')
plt.title('Prediction Error Distribution')
plt.grid(True)
plt.show()
```

## Домашнее задание

### Задание 1: Разработка и обучение нейронной сети для классификации изображений

Используя набор данных Fashion MNIST:

1. Загрузите и подготовьте данные:
   - Нормализуйте значения пикселей
   - Разделите данные на обучающую, валидационную и тестовую выборки
   - Преобразуйте метки в формат one-hot encoding

2. Разработайте и обучите нейронную сеть:
   - Создайте модель с не менее чем 3 скрытыми слоями
   - Используйте различные функции активации
   - Примените методы регуляризации (Dropout, BatchNormalization)
   - Настройте оптимизатор и функцию потерь
   - Используйте callbacks для ранней остановки и снижения скорости обучения

3. Оцените качество модели:
   - Вычислите точность на тестовой выборке
   - Постройте матрицу ошибок
   - Визуализируйте примеры правильных и неправильных предсказаний

4. Проведите эксперименты:
   - Сравните различные архитектуры (количество слоев, нейронов)
   - Сравните различные оптимизаторы (SGD, Adam, RMSprop)
   - Проанализируйте влияние регуляризации на переобучение

5. Подготовьте отчет с результатами и выводами

### Задание 2: Эксперименты с различными архитектурами и гиперпараметрами

1. Выберите набор данных для классификации или регрессии

2. Проведите серию экспериментов, изменяя:
   - Количество скрытых слоев (1, 2, 3, 4)
   - Количество нейронов в слоях
   - Функции активации (ReLU, tanh, sigmoid, LeakyReLU)
   - Оптимизаторы (SGD, Adam, RMSprop)
   - Скорость обучения (learning rate)
   - Размер батча (batch size)

3. Для каждого эксперимента:
   - Обучите модель с одинаковым количеством эпох
   - Запишите метрики качества на валидационной выборке
   - Визуализируйте процесс обучения (графики точности и функции потерь)

4. Проанализируйте результаты:
   - Определите оптимальную архитектуру и гиперпараметры
   - Объясните влияние различных параметров на качество модели
   - Сделайте выводы о лучших практиках для данной задачи

5. Подготовьте отчет с результатами и рекомендациями

## Ресурсы и материалы

### Основные ресурсы
- [Документация TensorFlow](https://www.tensorflow.org/api_docs)
- [Документация Keras](https://keras.io/api/)
- [Руководство по TensorFlow](https://www.tensorflow.org/guide)
- [Руководство по Keras](https://keras.io/guides/)

### Дополнительные материалы
- [Книга "Deep Learning with Python" by François Chollet](https://www.manning.com/books/deep-learning-with-python)
- [Курс "Deep Learning Specialization" на Coursera](https://www.coursera.org/specializations/deep-learning)
- [Статья "Understanding Neural Networks"](https://towardsdatascience.com/understanding-neural-networks-19020b758230)
- [Статья "A Visual Guide to Activation Functions"](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/)
- [Статья "Dropout: A Simple Way to Prevent Neural Networks from Overfitting"](http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)

### Наборы данных для практики
- [MNIST Dataset](http://yann.lecun.com/exdb/mnist/)
- [Fashion MNIST Dataset](https://github.com/zalandoresearch/fashion-mnist)
- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)
- [Boston Housing Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html)


Вот основные парадигмы машинного обучения с четкими различиями и примерами:

### 1. Обучение с учителем (Supervised Learning)
**Суть:** Модель учится на размеченных данных (известны и входные данные X, и правильные ответы Y).

**Типы задач:**
- **Классификация** (предсказание категории)
  - Пример: Определение спама в email (классы "спам"/"не спам")
  ```python
  from sklearn.ensemble import RandomForestClassifier
  model = RandomForestClassifier()
  model.fit(X_train, y_train)  # X_train - тексты писем, y_train - метки
  ```

- **Регрессия** (предсказание числа)
  - Пример: Прогноз цены дома по его параметрам
  ```python
  from sklearn.linear_model import LinearRegression
  model = LinearRegression()
  model.fit(X_train, y_train)  # X_train - характеристики домов, y_train - цены
  ```

**Когда использовать:** Когда есть исторические данные с правильными ответами.

---

### 2. Обучение без учителя (Unsupervised Learning)
**Суть:** Модель ищет паттерны в данных без готовых ответов (только X, без Y).

**Типы задач:**
- **Кластеризация**
  - Пример: Сегментация клиентов по поведению
  ```python
  from sklearn.cluster import KMeans
  kmeans = KMeans(n_clusters=3)
  kmeans.fit(X)  # X - данные о покупках
  ```

- **Понижение размерности**
  - Пример: Визуализация многомерных данных
  ```python
  from sklearn.decomposition import PCA
  pca = PCA(n_components=2)
  reduced_data = pca.fit_transform(X)
  ```

**Когда использовать:** Для исследования данных или когда разметка недоступна.

---

### 3. Обучение с подкреплением (Reinforcement Learning)
**Суть:** Агент учится через взаимодействие со средой, получая награды за правильные действия.

**Ключевые компоненты:**
- **Агент** (например, ИИ для игры)
- **Среда** (игровое поле)
- **Награда** (очки за правильные ходы)

**Пример:** Обучение бота для игры в шахматы
```python
# Псевдокод для Q-learning
q_table = defaultdict(float)
state = env.reset()
action = select_best_action(q_table, state)
next_state, reward, done = env.step(action)
q_table[state, action] += learning_rate * (reward + discount_factor * max(q_table[next_state]) - q_table[state, action])
```

**Когда использовать:** Для задач с последовательным принятием решений (игры, робототехника).

---

### Сравнение в таблице
| Параметр               | С учителем          | Без учителя         | С подкреплением     |
|------------------------|---------------------|---------------------|---------------------|
| **Данные**             | X и Y               | Только X            | Взаимодействие со средой |
| **Цель**               | Предсказать Y       | Найти паттерны      | Максимизировать награду |
| **Примеры алгоритмов** | Random Forest, SVM  | K-means, PCA        | Q-Learning, PPO     |
| **Сложность**          | Средняя             | Низкая              | Очень высокая       |

---

### Как выбрать?
1. Есть размеченные данные? → **Supervised**
2. Нужно исследовать данные? → **Unsupervised**
3. Есть среда с обратной связью? → **Reinforcement**

Для вашего бота анализа игроков:
- Если хотите **предсказывать PS** на основе истории → Supervised (регрессия)
- Если нужно **группировать игроков** по стилю игры → Unsupervised (кластеризация)
### Кодирование категориальных признаков

Когда мы работаем с моделями машинного обучения, важно понимать, что большинство алгоритмов не могут обрабатывать текстовые данные напрямую. Например, категории, такие как "Новичок" и "Профи", представляют собой текстовые метки, которые необходимо преобразовать в числовой формат, чтобы модели могли их использовать. Одним из наиболее распространенных методов для этого является **One-Hot Encoding**.

#### Проблема

Модели машинного обучения, такие как линейные регрессии, деревья решений и нейронные сети, требуют числовых входных данных. Если у вас есть категориальные признаки (например, "Новичок", "Профи"), их необходимо преобразовать в числовой формат.

#### One-Hot Encoding

**One-Hot Encoding** — это метод, который создает бинарные (0 или 1) колонки для каждого уникального значения в категориальном признаке. Это позволяет моделям различать категории, не вводя порядковую зависимость между ними.

##### Пример

Предположим, у вас есть следующий DataFrame с колонкой `rank`:

| rank     |
|----------|
| Новичок  |
| Профи    |
| Новичок  |
| Профи    |
| Эксперт  |

После применения One-Hot Encoding, DataFrame будет выглядеть следующим образом:

| rank_Новичок | rank_Профи | rank_Эксперт |
|--------------|------------|---------------|
| 1            | 0          | 0             |
| 0            | 1          | 0             |
| 1            | 0          | 0             |
| 0            | 1          | 0             |
| 0            | 0          | 1             |

Здесь:
- `rank_Новичок` — колонка, которая принимает значение 1, если `rank` равен "Новичок", и 0 в противном случае.
- `rank_Профи` — колонка, которая принимает значение 1, если `rank` равен "Профи", и 0 в противном случае.
- `rank_Эксперт` — колонка, которая принимает значение 1, если `rank` равен "Эксперт", и 0 в противном случае.

##### Кодирование в Python

Вы можете использовать библиотеку Pandas для выполнения One-Hot Encoding следующим образом:

```python
import pandas as pd

# Создаем DataFrame
data = {'rank': ['Новичок', 'Профи', 'Новичок', 'Профи', 'Эксперт']}
df = pd.DataFrame(data)

# Применяем One-Hot Encoding
df_encoded = pd.get_dummies(df['rank'], prefix='rank')

# Объединяем с исходным DataFrame
df = pd.concat([df, df_encoded], axis=1)

print(df)
```

### Заключение

One-Hot Encoding — это мощный метод для преобразования категориальных признаков в числовой формат, который позволяет моделям машинного обучения эффективно работать с текстовыми данными. Этот метод помогает избежать введения порядковых зависимостей между категориями и делает данные более понятными для алгоритмов. Если у вас есть дополнительные вопросы или вам нужно больше информации, не стесняйтесь спрашивать!



---
## One hot кодирование
При one-hot кодировании из категориального признака с k разными значениями мы получаем k новых бинарных (0/1) признаков. Например, пусть есть столбец Color с тремя возможными значениями:

```python:1.txt
import pandas as pd

df = pd.DataFrame({
    'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']
})
```

Без `drop_first`:

```python:1.txt
pd.get_dummies(df, columns=['Color'], drop_first=False)
```

 даст:

```
   Color_Blue  Color_Green  Color_Red
0           0            0          1
1           0            1          0
2           1            0          0
3           0            1          0
4           0            0          1
```

Здесь три столбца. Но между ними есть линейная зависимость:

    Color_Blue + Color_Green + Color_Red == 1  (для каждой строки)

Это и есть мультиколлинеарность в простом виде: одна из колонок выражается как линейная комбинация остальных.  
Почему это плохо?  
- В линейной регрессии или других моделях, требующих обращения матриц (например, OLS), матрица признаков должна быть полного ранга (не сингулярной).  
- Если признаки линейно зависимы – матрица X^T X не обратима, модель сойдётся некорректно или выдаст бесконечные коэффициенты.  
- Даже в более «стойких» методах коллинеарность ухудшает стабильность оценки и увеличивает дисперсию коэффициентов.

Чтобы этого избежать, принято «отбросить» один из бинарных столбцов. Если убрать, скажем, `Color_Blue`, то:

```python:1.txt
pd.get_dummies(df, columns=['Color'], drop_first=True)
```

даст:

```
   Color_Green  Color_Red
0            0          1
1            1          0
2            0          0
3            1          0
4            0          1
```

Теперь:

- `Color_Green` и `Color_Red` полностью описывают категорию:  
  • если оба 0 → значит был Blue (базовый уровень),  
  • если `(1,0)` → Green,  
  • если `(0,1)` → Red.  
- У нас осталось 2 столбца, и они уже не линейно зависимы.

Параметр `drop_first=True` автоматом удаляет первый по алфавиту (или по порядку во входном столбце) новый бинарный признак.  

Итоги:

1. «Первый» битовый признак становится базовым уровнем (reference level).  
2. Убирая один признак, мы снимаем линейную зависимость между столбцами, устраняя мультиколлинеарность.  
3. В моделях линейной регрессии так понятнее:  
   - коэффициент при каждом оставшемся бинарном признаке показывает изменение целевой переменной относительно базового уровня.
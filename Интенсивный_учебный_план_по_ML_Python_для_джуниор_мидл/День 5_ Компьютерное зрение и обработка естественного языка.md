# День 5: Компьютерное зрение и обработка естественного языка

## Расписание дня

| Время | Активность |
|-------|------------|
| 09:00 - 10:30 | Теория: Основы компьютерного зрения и сверточные нейронные сети |
| 10:30 - 10:45 | Перерыв |
| 10:45 - 12:15 | Теория: Основы обработки естественного языка |
| 12:15 - 13:15 | Обед |
| 13:15 - 15:00 | Практика: Реализация CNN для классификации изображений |
| 15:00 - 15:15 | Перерыв |
| 15:15 - 17:00 | Практика: Обработка текстовых данных и анализ тональности |
| 17:00 - 18:00 | Работа над домашним заданием и вопросы |

## Теоретические материалы

### Основы компьютерного зрения

#### Введение в компьютерное зрение
- Определение и задачи компьютерного зрения
- История развития компьютерного зрения
- Основные области применения
- Современные тенденции и достижения

#### Представление изображений в компьютере
- Цветовые модели (RGB, HSV, Grayscale)
- Пиксели и каналы
- Матричное представление изображений
- Форматы изображений и их особенности

#### Предобработка изображений
- Изменение размера и обрезка
- Нормализация и стандартизация
- Фильтрация и сглаживание
- Повышение контрастности
- Аугментация данных для обучения

### Сверточные нейронные сети (CNN)

#### Архитектура сверточных нейронных сетей
- Сверточные слои (Convolutional layers)
- Функции активации в CNN
- Слои подвыборки (Pooling layers)
- Полносвязные слои (Fully connected layers)
- Dropout в CNN

#### Операция свертки
- Принцип работы свертки
- Фильтры и карты признаков
- Шаг (stride) и дополнение (padding)
- Глубина свертки и каналы

#### Слои подвыборки
- Max pooling
- Average pooling
- Global pooling
- Назначение и эффекты подвыборки

#### Классические архитектуры CNN
- LeNet-5
- AlexNet
- VGG
- ResNet
- Inception (GoogLeNet)
- MobileNet

#### Трансферное обучение
- Принципы трансферного обучения
- Предобученные модели (ImageNet)
- Fine-tuning предобученных моделей
- Feature extraction

### Основы обработки естественного языка (NLP)

#### Введение в NLP
- Определение и задачи NLP
- История развития NLP
- Основные области применения
- Современные тенденции и достижения

#### Предобработка текстовых данных
- Токенизация
- Удаление стоп-слов
- Стемминг и лемматизация
- Нормализация текста
- Обработка специальных символов и чисел

#### Представление текста
- Мешок слов (Bag of Words)
- TF-IDF (Term Frequency-Inverse Document Frequency)
- N-граммы
- One-hot encoding
- Word embeddings (Word2Vec, GloVe, FastText)

#### Рекуррентные нейронные сети (RNN)
- Принцип работы RNN
- Проблема исчезающего и взрывного градиента
- LSTM (Long Short-Term Memory)
- GRU (Gated Recurrent Unit)
- Двунаправленные RNN (Bidirectional RNN)

#### Трансформеры
- Архитектура трансформера
- Механизм внимания (Attention mechanism)
- Self-attention
- Позиционное кодирование
- Модели на основе трансформеров (BERT, GPT, T5)

## Практические задания

### Задание 1: Реализация CNN для классификации изображений

#### Установка и импорт библиотек
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.utils import plot_model

print(f"TensorFlow version: {tf.__version__}")
print(f"Keras version: {keras.__version__}")
```

#### Загрузка и подготовка данных CIFAR-10
```python
# Загрузка набора данных CIFAR-10
from tensorflow.keras.datasets import cifar10

# Загрузка данных
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# Названия классов
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 
               'dog', 'frog', 'horse', 'ship', 'truck']

# Просмотр формы данных
print(f"X_train shape: {X_train.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_test shape: {y_test.shape}")

# Визуализация нескольких примеров
plt.figure(figsize=(10, 10))
for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.imshow(X_train[i])
    plt.title(class_names[y_train[i][0]])
    plt.axis('off')
plt.tight_layout()
plt.show()

# Предобработка данных
# 1. Нормализация значений пикселей
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

# 2. Преобразование меток в one-hot encoding
y_train_onehot = keras.utils.to_categorical(y_train, 10)
y_test_onehot = keras.utils.to_categorical(y_test, 10)
```

#### Создание и обучение простой CNN
```python
# Создание модели CNN
model = keras.Sequential([
    # Первый сверточный блок
    layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)),
    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Второй сверточный блок
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Полносвязные слои
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Компиляция модели
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Вывод структуры модели
model.summary()

# Визуализация архитектуры модели
plot_model(model, to_file='cnn_architecture.png', show_shapes=True, show_layer_names=True)

# Настройка callbacks
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.2,
    patience=5
)

# Обучение модели
history = model.fit(
    X_train, y_train_onehot,
    epochs=50,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Оценка модели на тестовой выборке
test_loss, test_acc = model.evaluate(X_test, y_test_onehot)
print(f"Test accuracy: {test_acc:.4f}")
```

#### Визуализация процесса обучения и результатов
```python
# Визуализация процесса обучения
plt.figure(figsize=(12, 5))

# График точности
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# График функции потерь
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Получение предсказаний
y_pred_proba = model.predict(X_test)
y_pred = np.argmax(y_pred_proba, axis=1)
y_test_flat = y_test.flatten()

# Визуализация некоторых предсказаний
plt.figure(figsize=(12, 12))
for i in range(25):
    plt.subplot(5, 5, i+1)
    plt.imshow(X_test[i])
    pred_label = y_pred[i]
    true_label = y_test_flat[i]
    color = 'green' if pred_label == true_label else 'red'
    plt.title(f"P: {class_names[pred_label]}\nT: {class_names[true_label]}", color=color)
    plt.axis('off')
plt.tight_layout()
plt.show()

# Матрица ошибок
from sklearn.metrics import confusion_matrix, classification_report
cm = confusion_matrix(y_test_flat, y_pred)

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names, yticklabels=class_names)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Отчет о классификации
print(classification_report(y_test_flat, y_pred, target_names=class_names))
```

### Задание 2: Использование предобученных моделей (Transfer Learning)

```python
# Загрузка предобученной модели VGG16
from tensorflow.keras.applications import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Загрузка базовой модели VGG16 (без верхних слоев)
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Заморозка весов базовой модели
base_model.trainable = False

# Создание новой модели на основе VGG16
model_tl = keras.Sequential([
    base_model,
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Компиляция модели
model_tl.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Вывод структуры модели
model_tl.summary()

# Подготовка данных для предобученной модели
# Изменение размера изображений и аугментация
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2
)

test_datagen = ImageDataGenerator(rescale=1./255)

# Для примера используем небольшой набор данных (например, первые 5000 изображений)
X_train_sample = np.array([tf.image.resize(img, (224, 224)) for img in X_train[:5000]])
y_train_sample = y_train_onehot[:5000]

X_test_sample = np.array([tf.image.resize(img, (224, 224)) for img in X_test[:1000]])
y_test_sample = y_test_onehot[:1000]

# Обучение модели
history_tl = model_tl.fit(
    X_train_sample, y_train_sample,
    epochs=10,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# Оценка модели на тестовой выборке
test_loss_tl, test_acc_tl = model_tl.evaluate(X_test_sample, y_test_sample)
print(f"Test accuracy (Transfer Learning): {test_acc_tl:.4f}")
```

### Задание 3: Обработка текстовых данных и векторизация

```python
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

# Загрузка необходимых ресурсов NLTK
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Пример текстовых данных
texts = [
    "Machine learning is a subset of artificial intelligence that provides systems the ability to automatically learn and improve from experience.",
    "Deep learning is part of a broader family of machine learning methods based on artificial neural networks.",
    "Natural language processing is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language.",
    "Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos."
]

labels = ["machine_learning", "deep_learning", "nlp", "computer_vision"]

# Предобработка текста
def preprocess_text(text, remove_stopwords=True, stemming=False, lemmatization=True):
    # Приведение к нижнему регистру
    text = text.lower()
    
    # Токенизация
    tokens = word_tokenize(text)
    
    # Удаление стоп-слов
    if remove_stopwords:
        stop_words = set(stopwords.words('english'))
        tokens = [token for token in tokens if token not in stop_words]
    
    # Стемминг
    if stemming:
        stemmer = PorterStemmer()
        tokens = [stemmer.stem(token) for token in tokens]
    
    # Лемматизация
    if lemmatization:
        lemmatizer = WordNetLemmatizer()
        tokens = [lemmatizer.lemmatize(token) for token in tokens]
    
    return tokens

# Применение предобработки к текстам
preprocessed_texts = [preprocess_text(text) for text in texts]

# Вывод результатов предобработки
for i, (original, preprocessed) in enumerate(zip(texts, preprocessed_texts)):
    print(f"Original ({labels[i]}): {original}")
    print(f"Preprocessed: {' '.join(preprocessed)}")
    print()

# Векторизация текста с помощью Bag of Words
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer()
X_bow = vectorizer.fit_transform([' '.join(tokens) for tokens in preprocessed_texts])

# Вывод результатов векторизации
print("Bag of Words Representation:")
print(f"Shape: {X_bow.shape}")
print("Feature names (vocabulary):")
print(vectorizer.get_feature_names_out())
print("\nBag of Words Matrix:")
print(X_bow.toarray())

# Векторизация текста с помощью TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform([' '.join(tokens) for tokens in preprocessed_texts])

# Вывод результатов векторизации
print("\nTF-IDF Representation:")
print(f"Shape: {X_tfidf.shape}")
print("Feature names (vocabulary):")
print(tfidf_vectorizer.get_feature_names_out())
print("\nTF-IDF Matrix:")
print(X_tfidf.toarray())
```

### Задание 4: Реализация модели для анализа тональности текста

```python
# Загрузка набора данных для анализа тональности (например, IMDB)
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Загрузка данных
max_features = 10000  # Размер словаря
maxlen = 200  # Максимальная длина последовательности

(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)

# Просмотр формы данных
print(f"X_train shape: {len(X_train)}")
print(f"X_test shape: {len(X_test)}")

# Преобразование последовательностей в одинаковую длину
X_train_pad = pad_sequences(X_train, maxlen=maxlen)
X_test_pad = pad_sequences(X_test, maxlen=maxlen)

print(f"X_train_pad shape: {X_train_pad.shape}")
print(f"X_test_pad shape: {X_test_pad.shape}")

# Получение словаря для отображения индексов в слова
word_index = imdb.get_word_index()
reverse_word_index = {value: key for key, value in word_index.items()}

# Функция для декодирования отзыва
def decode_review(encoded_review):
    return ' '.join([reverse_word_index.get(i - 3, '?') for i in encoded_review])

# Вывод нескольких примеров
for i in range(3):
    print(f"Review {i+1}:")
    print(decode_review(X_train[i]))
    print(f"Sentiment: {'Positive' if y_train[i] == 1 else 'Negative'}")
    print()

# Создание модели для анализа тональности
model_sentiment = keras.Sequential([
    layers.Embedding(max_features, 128, input_length=maxlen),
    layers.Conv1D(128, 5, activation='relu'),
    layers.GlobalMaxPooling1D(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

# Компиляция модели
model_sentiment.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Вывод структуры модели
model_sentiment.summary()

# Обучение модели
history_sentiment = model_sentiment.fit(
    X_train_pad, y_train,
    epochs=10,
    batch_size=128,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=1
)

# Оценка модели на тестовой выборке
test_loss, test_acc = model_sentiment.evaluate(X_test_pad, y_test)
print(f"Test accuracy: {test_acc:.4f}")

# Визуализация процесса обучения
plt.figure(figsize=(12, 5))

# График точности
plt.subplot(1, 2, 1)
plt.plot(history_sentiment.history['accuracy'], label='Train')
plt.plot(history_sentiment.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# График функции потерь
plt.subplot(1, 2, 2)
plt.plot(history_sentiment.history['loss'], label='Train')
plt.plot(history_sentiment.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# Предсказания и анализ результатов
y_pred_proba = model_sentiment.predict(X_test_pad)
y_pred = (y_pred_proba > 0.5).astype(int).flatten()

# Матрица ошибок
from sklearn.metrics import confusion_matrix, classification_report

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negative', 'Positive'],
            yticklabels=['Negative', 'Positive'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# Отчет о классификации
print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))

# Анализ ошибок
errors = []
for i in range(len(y_test)):
    if y_test[i] != y_pred[i]:
        errors.append((i, y_test[i], y_pred[i], y_pred_proba[i][0]))

# Вывод нескольких ошибочных предсказаний
print("Examples of misclassifications:")
for i, true, pred, prob in errors[:5]:
    print(f"Review: {decode_review(X_test[i][:50])}...")
    print(f"True sentiment: {'Positive' if true == 1 else 'Negative'}")
    print(f"Predicted sentiment: {'Positive' if pred == 1 else 'Negative'} (probability: {prob:.4f})")
    print()
```

## Домашнее задание

### Задание 1: Решение задачи классификации изображений с использованием CNN

Используя набор данных CIFAR-10 или аналогичный:

1. Разработайте и обучите сверточную нейронную сеть для классификации изображений:
   - Создайте архитектуру с не менее чем 3 сверточными блоками
   - Используйте различные методы регуляризации (Dropout, BatchNormalization)
   - Примените аугментацию данных для улучшения обобщающей способности
   - Настройте оптимизатор и функцию потерь
   - Используйте callbacks для ранней остановки и снижения скорости обучения

2. Примените трансферное обучение:
   - Используйте предобученную модель (VGG16, ResNet50, MobileNet и т.д.)
   - Заморозьте веса предобученной части
   - Добавьте собственные слои для классификации
   - Обучите модель на вашем наборе данных

3. Сравните результаты:
   - Оцените точность обеих моделей на тестовой выборке
   - Сравните время обучения и размер моделей
   - Проанализируйте ошибки классификации
   - Визуализируйте активации сверточных слоев

4. Подготовьте отчет с результатами и выводами

### Задание 2: Реализация модели для анализа тональности отзывов

Используя набор данных с отзывами (например, Amazon Reviews, Yelp Reviews):

1. Выполните предобработку текстовых данных:
   - Токенизация
   - Удаление стоп-слов
   - Лемматизация или стемминг
   - Векторизация (Bag of Words, TF-IDF или Word Embeddings)

2. Разработайте и обучите модель для анализа тональности:
   - Создайте модель на основе RNN (LSTM или GRU)
   - Используйте предобученные word embeddings (Word2Vec, GloVe)
   - Настройте гиперпараметры модели
   - Обучите модель на подготовленных данных

3. Оцените качество модели:
   - Вычислите точность, полноту и F1-меру
   - Постройте матрицу ошибок
   - Проанализируйте ошибки классификации
   - Визуализируйте важность слов для классификации

4. Проведите эксперименты:
   - Сравните различные архитектуры (RNN, CNN для текста, трансформеры)
   - Сравните различные методы векторизации
   - Проанализируйте влияние предобработки на качество модели

5. Подготовьте отчет с результатами и выводами

## Ресурсы и материалы

### Основные ресурсы
- [Документация TensorFlow по CNN](https://www.tensorflow.org/tutorials/images/cnn)
- [Документация Keras по предобученным моделям](https://keras.io/api/applications/)
- [Документация TensorFlow по NLP](https://www.tensorflow.org/text)
- [Документация NLTK](https://www.nltk.org/)
- [Документация spaCy](https://spacy.io/api/doc)

### Дополнительные материалы
- [Книга "Deep Learning for Computer Vision" by Rajalingappaa Shanmugamani](https://www.packtpub.com/product/deep-learning-for-computer-vision/9781788295628)
- [Курс "Convolutional Neural Networks" на Coursera](https://www.coursera.org/learn/convolutional-neural-networks)
- [Статья "A Comprehensive Guide to Convolutional Neural Networks"](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)
- [Книга "Natural Language Processing with Python" by Steven Bird, Ewan Klein, and Edward Loper](https://www.nltk.org/book/)
- [Статья "Understanding LSTM Networks"](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [Статья "The Illustrated Transformer"](http://jalammar.github.io/illustrated-transformer/)

### Наборы данных для практики
- [CIFAR-10 и CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html)
- [ImageNet](http://www.image-net.org/)
- [IMDB Movie Reviews](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)
- [Amazon Reviews](https://www.kaggle.com/bittlingmayer/amazonreviews)
- [Yelp Reviews](https://www.kaggle.com/yelp-dataset/yelp-dataset)

